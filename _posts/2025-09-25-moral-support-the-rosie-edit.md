---
title: "Moral Support, the Rosie edit"
published: true
---

![Moral Support Header](/assets/img/moral-support-header.png)

This week's feature is from the brain and keyboard of my fellow Deltastring founder Rosie Elliott-Welch. 

## Where is the humanity?

Plenty of people in my life have done their time working customer facing roles, in one form or another, so when they have a bad experience with a business it comes up in conversation. This has only increased now I'm running my business Deltastring and working with brands my friends have heard of!

Increasingly the object of their ire is an AI bot which stood between customer and human agent. It's frustrating when you feel like this is a quick solve for a human but the bot isn't quite grasping what you're asking.

Most people are well aware that if they repeatedly ask for a human it will usually be escalated. I've been considering the implications for the human agents. There could be hundreds of messages over multiple threads before the point where real eyeballs look at the situation.I realised that it turns regular agents into unwilling supervisors. Typing I WANT A HUMAN to a chat bot is the equivalent of asking for the manager. We've rearranged the hierarchy without consideration for how this impacts experiences.

![Human needed!](https://deltastring.com/assets/img/need-a-human.png)
*Just get a human person please!*

After my years as a retailer I know that some proportion of people will always demand an escalation even after good service, even when their problems were solved. Sales assistants can usually rapidly identify customer intention. Training new hires includes reminding how humans communicate with tone to a greater degree than words themselves. I’m sure most of you have been shown a pie chart of Albert Mehrabian's 7-38-55 Communication model, however misinterpreted that tool is! No AI can understand these nuances quite like a human can.

When the bot gets it wrong, or the customer expects human service at the point of contact we are creating a point of contention before the issue is even raised, and when it’s escalated the agent has to smooth over the experience the customer has had at the very beginning of the conversation. We’re starting on the back foot.I think we have to acknowledge that if AI agents are implemented that we are expecting agents to deal with customers who will resent having to interact with it. Although basic queries may be answered by not validating customers with personal service there is now extra escalation created by simply having AI agents.

Ultimately, every business has to balance a range of opposing demands. Providing support is expensive, but it provides valuable insight to steer product development. Customers want more, faster, better, but they don't want prices to shoot up. Businesses need to be on top of technology, and avoid getting left behind by more agile competitors. Where is the tipping point? Ask your agents, your customers are telling them! 